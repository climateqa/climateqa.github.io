"use strict";(self.webpackChunkclimateqa=self.webpackChunkclimateqa||[]).push([[5894],{6042:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"openalex","metadata":{"permalink":"/blog/openalex","source":"@site/blog/2024-03-25-openalex/index.md","title":"ClimateQ&A v1.4 now has access to 250 million research papers from OpenAlex.org","description":"ClimateQ&A version 1.4 introduces an experimental feature with the access to OpenAlex.org, providing users with over 250 million scientific papers in addition to existing IPCC and IPBES reports. This update responds to the demand for wider access to recent scientific research, allowing for exploration of topics not covered in standard reports. The feature includes algorithms for searching, ranking, and summarizing relevant papers, aiming to facilitate direct access to current scientific knowledge.","date":"2024-03-25T00:00:00.000Z","formattedDate":"March 25, 2024","tags":[{"label":"ClimateQ&A","permalink":"/blog/tags/climate-q-a"},{"label":"OpenAlex.org","permalink":"/blog/tags/open-alex-org"}],"readingTime":4.49,"hasTruncateMarker":false,"authors":[{"name":"Th\xe9o ALVES DA COSTA","url":"theo.alvesdacosta@ekimetrics.com"}],"frontMatter":{"slug":"openalex","title":"ClimateQ&A v1.4 now has access to 250 million research papers from OpenAlex.org","author":"Th\xe9o ALVES DA COSTA","author_url":"theo.alvesdacosta@ekimetrics.com","header_image_url":"img/screenshots/openalex.png","tags":["ClimateQ&A","OpenAlex.org"],"description":"ClimateQ&A version 1.4 introduces an experimental feature with the access to OpenAlex.org, providing users with over 250 million scientific papers in addition to existing IPCC and IPBES reports. This update responds to the demand for wider access to recent scientific research, allowing for exploration of topics not covered in standard reports. The feature includes algorithms for searching, ranking, and summarizing relevant papers, aiming to facilitate direct access to current scientific knowledge.","keywords":["ClimateQ&A","Climate Change"]},"unlisted":false,"nextItem":{"title":"Our first research paper on the meta-analysis of our user questions","permalink":"/blog/paper"}},"content":"## Abstract\\r\\n\\r\\nThe latest version of ClimateQ&A introduces an exciting experimental feature that significantly expands its capability to provide up-to-date scientific knowledge. Responding to a high demand from its user base, ClimateQ&A now offers access to over 250 million scientific papers through integration with OpenAlex.org, alongside its existing repertoire of pre-recorded IPCC and IPBES reports. This enhancement is driven by the necessity to keep abreast of the latest scientific advancements and to cover a wider range of social and environmental topics not addressed in the standard IPCC and IPBES assessments. The feature, currently in beta, is accessible through the \\"Papers\\" tab, separate from the main chat interface, facilitating a direct and efficient exploration of current scientific literature.\\r\\n\\r\\n## ClimateQ&A v1.4 now has access to 250 million research papers from OpenAlex.org\\r\\n:::warning[Experimental]\\r\\n\\r\\nThis feature is still in the experimental phase and may undergo changes in the coming months.\\r\\nIn particular, now it\'s still super slow on the version deployed online.\\r\\n\\r\\n:::\\r\\n\\r\\nOne of the most requested features for ClimateQ&A was the ability to access more scientific documents than the pre-recorded reports (IPCC, IPBES). This demand is driven by several reasons:\\r\\n- To update the knowledge and identify knowledge gaps on the new scientific advances since the previous assessments.\\r\\n- To access information on social/environmental topics not mentioned in the IPCC and IPBES reports.\\r\\n\\r\\nDuring our work, we discovered [OpenAlex.org](https://openalex.org/), a platform that provides access to 250 million scientific papers, along with API access.\\r\\n\\r\\nTherefore, version 1.4 of ClimateQ&A introduces an experimental feature for accessing OpenAlex.org in addition to the existing scientific documents.\\r\\n\\r\\nFor the time being, this feature is separate from the chat interface during the beta phase and is accessible under the **Papers** tab.\\r\\n\\r\\n### Example 1 - What are the impacts of PFAS on babies?\\r\\nFor instance, let\'s explore the response to a question regarding \\"forever chemicals\\" (PFAS), a topic not addressed in the existing corpus (IPCC, IPBES). It\'s necessary to specify the question \\"What are the impacts of PFAS on babies?\\" along with keywords for searching on OpenAlex.org, here being \\"PFAS\\". It\'s also possible to set a minimum publication date (for instance, to only include recent publications).\\r\\n\\r\\nSimilar to the ClimateQ&A chat feature, we find in the **Summary** tab a synthesis of the information gleaned from the abstracts of the 15 most relevant papers to address the question.\\r\\n\\r\\n![](./fig3.png)\\r\\n\\r\\nIn the **Relevant Papers** tab, a table of the 100 most pertinent publications is displayed, including all necessary information (title, abstract, DOI, URL, whether the paper is open access, etc.), with the publications sorted by relevance to the question.\\r\\n\\r\\n![](./fig2.png)\\r\\n\\r\\nFinally, the **Citations Network** tab shows a graph of the 100 publications connected by citations. In other words, an arrow indicates one paper citing another. This allows us to visually identify the most significant papers (the most cited) and the clusters that emerge to reveal different research themes. In this network, the size corresponds to the PageRank value of a node, meaning it\'s a measure of the paper\'s importance based on the number and quality of citations it receives. The more high-quality papers that cite a given paper, the higher its PageRank, indicating its influence and significance in the field. And the color indicates the paper\'s relevance to the question (with red being more relevant). This feature is directly inspired from https://www.connectedpapers.com/.\\r\\n\\r\\n![](./fig1.png)\\r\\n\\r\\nIt\'s also very easy to follow the links to the referenced papers to directly open OpenAlex.org and continue the research.\\r\\n\\r\\n![](./fig4.png)\\r\\n\\r\\n### Example 2 - What are the impacts of deep sea mining?\\r\\n\\r\\nLet\'s examine another question, \\"What are the impacts of deep sea mining?\\", a subject seldom mentioned in the IPBES reports. Applying a filter for publications after 2015, we clearly see the most significant publications at the center of the network.\\r\\n\\r\\n![](./fig5.png)\\r\\n\\r\\n## How it Works\\r\\n\\r\\nA simplified operation is detailed in the schema below:\\r\\n\\r\\n![](./fig6.png)\\r\\n\\r\\n- From a user\'s question, a list of keywords is extracted using an LLM (Large Language Model). The user can also manually adjust the keywords. Since OpenAlex does not operate with semantic search, extracting a comprehensive set of relevant keywords is necessary to find papers likely to answer the question.\\r\\n- We search for 100 relevant papers based on the keywords using OpenAlex.org\'s API.\\r\\n- These papers are then sorted based on their relevance to the question (rather than to the keywords) using a cross-encoder model (also known as ReRanker, here we use [Mixed Bread AI\'s ReRank V1](https://www.mixedbread.ai/blog/mxbai-rerank-v1)).\\r\\n- We select the 15 most relevant papers with the top reranking scores.\\r\\n- A summarized response is synthesized from the abstracts of these 15 papers using an LLM and a prompt similar to that used in the chat.\\r\\n\\r\\n\\r\\n## Next Steps\\r\\n\\r\\nThis is a new experimental feature, with many ideas for improvement:\\r\\n\\r\\n- Currently, the model remains quite slow on CPU (about 30 seconds to answer a question), leaving ample room for optimization.\\r\\n- After a testing phase, we will directly add OpenAlex.org as a documentary source in the Configuration tab to include the search (if relevant) additionally.\\r\\n- It is also possible to delve deeper into citation networks to retrieve more relevant publications.\\r\\n- It would be fantastic to link the scientific publications that contributed to the IPCC and IPBES assessments to provide updates to help scientists sift through the most relevant publications to update knowledge.\\r\\n\\r\\n## Code\\r\\n\\r\\nThe open source code is available at https://huggingface.co/spaces/Ekimetrics/climate-question-answering/blob/main/climateqa/papers/openalex.py"},{"id":"paper","metadata":{"permalink":"/blog/paper","source":"@site/blog/2024-03-18-publication/index.md","title":"Our first research paper on the meta-analysis of our user questions","description":"Meta-analysis of the first 3500 questions asked to ClimateQ&A. This research paper investigates public views on climate change and biodiversity loss by analyzing questions asked to the ClimateQ&A platform.","date":"2024-03-18T00:00:00.000Z","formattedDate":"March 18, 2024","tags":[{"label":"ClimateQ&A","permalink":"/blog/tags/climate-q-a"},{"label":"OpenAlex.org","permalink":"/blog/tags/open-alex-org"}],"readingTime":1.06,"hasTruncateMarker":false,"authors":[{"name":"Th\xe9o ALVES DA COSTA","url":"theo.alvesdacosta@ekimetrics.com"}],"frontMatter":{"slug":"paper","title":"Our first research paper on the meta-analysis of our user questions","author":"Th\xe9o ALVES DA COSTA","author_url":"theo.alvesdacosta@ekimetrics.com","header_image_url":"img/screenshots/treemap.png","tags":["ClimateQ&A","OpenAlex.org"],"description":"Meta-analysis of the first 3500 questions asked to ClimateQ&A. This research paper investigates public views on climate change and biodiversity loss by analyzing questions asked to the ClimateQ&A platform.","keywords":["ClimateQ&A","Climate Change"]},"unlisted":false,"prevItem":{"title":"ClimateQ&A v1.4 now has access to 250 million research papers from OpenAlex.org","permalink":"/blog/openalex"},"nextItem":{"title":"ClimateQ&A now features image interpretation","permalink":"/blog/multimodality"}},"content":"https://arxiv.org/abs/2403.14709 \\r\\n\\r\\nThis research paper investigates public views on climate change and biodiversity loss by analyzing questions asked to the ClimateQ&A platform. ClimateQ&A is a conversational agent that uses LLMs to respond to queries based on over 14,000 pages of scientific literature from the IPCC and IPBES reports. Launched online in March 2023, the tool has gathered over 30,000 questions, mainly from a French audience. Its chatbot interface allows for the free formulation of questions related to nature*. While its main goal is to make nature science more accessible, it also allows for the collection and analysis of questions and their themes. Unlike traditional surveys involving closed questions, this novel method offers a fresh perspective on individual interrogations about nature. Running NLP clustering algorithms on a sample of 3,425 questions, we find that a significant 25.8% inquire about how climate change and biodiversity loss will affect them personally (e.g., where they live or vacation, their consumption habits) and the specific impacts of their actions on nature (e.g., transportation or food choices). This suggests that traditional methods of surveying may not identify all existing knowledge gaps, and that relying solely on IPCC and IPBES reports may not address all individual inquiries about climate and biodiversity, potentially affecting public understanding and action on these issues"},{"id":"multimodality","metadata":{"permalink":"/blog/multimodality","source":"@site/blog/2024-02-20-multimodality/index.md","title":"ClimateQ&A now features image interpretation","description":"ClimateQ&A version 1.3 introduces a new feature that enhances explanations with relevant images, significantly improving the interpretation of complex scientific reports from sources like the IPCC and IPBES. This feature, still in its experimental phase, leverages multimodal Large Language Models (LLMs) to index image descriptions into a search engine.","date":"2024-02-20T00:00:00.000Z","formattedDate":"February 20, 2024","tags":[{"label":"ClimateQ&A","permalink":"/blog/tags/climate-q-a"},{"label":"Multi-modality","permalink":"/blog/tags/multi-modality"}],"readingTime":3.38,"hasTruncateMarker":false,"authors":[{"name":"Th\xe9o ALVES DA COSTA","url":"theo.alvesdacosta@ekimetrics.com"}],"frontMatter":{"slug":"multimodality","title":"ClimateQ&A now features image interpretation","author":"Th\xe9o ALVES DA COSTA","author_url":"theo.alvesdacosta@ekimetrics.com","header_image_url":"img/screenshots/multimodal1.png","tags":["ClimateQ&A","Multi-modality"],"description":"ClimateQ&A version 1.3 introduces a new feature that enhances explanations with relevant images, significantly improving the interpretation of complex scientific reports from sources like the IPCC and IPBES. This feature, still in its experimental phase, leverages multimodal Large Language Models (LLMs) to index image descriptions into a search engine.","keywords":["ClimateQ&A","Climate Change"]},"unlisted":false,"prevItem":{"title":"Our first research paper on the meta-analysis of our user questions","permalink":"/blog/paper"}},"content":"## Abstract\\r\\n\\r\\nThe article discusses the innovative feature introduced in ClimateQ&A version 1.3, which enables the platform to display images alongside explanations, enhancing the user\'s understanding of complex scientific data. This feature is particularly useful for interpreting scientific reports, such as those from the IPCC and IPBES, which are rich in illustrative images designed to communicate scientific outcomes.\\r\\n\\r\\nTo implement this capability, the system indexes descriptions generated by multimodal Large Language Models (LLMs) into its search engine. For each image, the text before and after it is also captured to provide context, often necessary as the following text frequently acts as the image\'s caption. These descriptions are then generated using the GPT4Vision API and converted into vectors using the same embedding model as the text, making them searchable within the platform.\\r\\n\\r\\n## ClimateQ&A now features image interpretation\\r\\n:::warning[Experimental]\\r\\n\\r\\nThis feature is still in the experimental phase and may undergo changes in the coming months.\\r\\n\\r\\n:::\\r\\n\\r\\nStarting from [version 1.3](/docs/changelog/v1.3.0), ClimateQ&A now offers the capability to include images alongside explanations.\\r\\n\\r\\nThe image is displayed within the chat interface at the end of the message, as illustrated above.\\r\\n\\r\\nThis \\"image interpretation\\" feature was highly requested, given that scientific reports, such as those from the IPCC and IPBES, often incorporate numerous images designed to convey complex scientific results.\\r\\n\\r\\nOf course, not all inquiries will return images; the system only provides images when they are relevant to the explanation. Likely images from the referenced passages are accessible under the \\"Figures\\" tab.\\r\\n\\r\\n![](./fig2.png)\\r\\n\\r\\n## How it works\\r\\n\\r\\nTo implement this feature, we index descriptions generated by multimodal Large Language Models (LLMs) in our search engine.\\r\\n\\r\\n- For this initial release, during the phase of parsing scientific reports, we independently extracted each image (recalling that the document segmentation and parsing are performed using a YOLO model fine-tuned on the [DocLayNet dataset](https://github.com/DS4SD/DocLayNet)).\\r\\n- For each image, we also retrieve the preceding and following text. This is done to provide the multimodal LLM with the context in which the image is situated, assuming that the following text often serves as the image\'s caption.\\r\\n- We then use the GPT4-Vision API to generate descriptions from the image and context texts, using the following prompt:\\r\\n\\r\\n```\\r\\nYou are going to act as a caption generator. Your goal is NOT to generate a description of the image but more the summary and explanation it conveys. This description will be indexed in a search engine to be found based on a user query.\\r\\nThe images are drawn from the scientific reports of the IPCC on climate change and IPBES on biodiversity. So they are mostly complex science schemas.\\r\\nIf the image is weird, is a logo, looks like it has been wrongly cropped, or you don\'t know what it is, just return \\"unknown\\". \\r\\n\\r\\nI also provide the context for the image by giving you the text coming before the image in the document, and the text coming after the image (often a caption for the figure, so it may be useful).\\r\\n\\r\\nText before:\\r\\n{text_before}\\r\\n\\r\\n\\r\\nText after:\\r\\n{text_after}\\r\\n\\r\\nSummary:\\r\\n```\\r\\n- The description is then converted into vectors using the same embedding model as the text and added to the vector store to be searchable in the same way as text chunks.\\r\\n\\r\\n- Then, at the time of the final synthesis in the interface in response to a question, if one of the k retrieved chunks is an image, we display it at the end, while indicating to the user that the description was AI-generated.\\r\\n![](./fig1.png)\\r\\n\\r\\n\\r\\n## Next Steps\\r\\n\\r\\nThis is an initial, still experimental feature, but the results are already satisfactory enough to continue work in this direction. Our next steps to improve this feature include:\\r\\n\\r\\n- Expanding the number of images covered; currently, not all images are analyzed.\\r\\n- Better integrate images in the response and return several images.\\r\\n- Moving to an open-source model like LLaVA to experiment more quickly and allow for easier replication of experiments.\\r\\n- Directly indexing figures indexed by the IPCC (e.g., https://www.ipcc.ch/report/ar6/syr/figures/).\\r\\n- Improving the caption identification of our detection model."}]}')}}]);